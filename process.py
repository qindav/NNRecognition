# -*- coding: utf-8 -*-
"""process.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14LT6PoMEOjL9iVlm05JKCxKP5I79gqyt
"""

# Commented out IPython magic to ensure Python compatibility.
# Lots of headers, though some may be unused rn
import sys
import os
import shutil
import cv2
import math
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
from keras.preprocessing import image
import numpy as np
from keras.utils import np_utils
from skimage.transform import resize
from sklearn.model_selection import train_test_split
from keras.models import load_model
from glob import glob
from tqdm import tqdm

import keras
from keras.models import Sequential
from keras.applications.vgg16 import VGG16
from keras.layers import Dense, InputLayer, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D

from moviepy.editor import VideoFileClip

"""Part 1: Partition the video into frames and save within a temp folder."""
videoFile = sys.argv[1]
# The input function doesn't seem to work on colab, manually editing the file name for now.
# input(videoFile)

# Breaks the video by frames for training set
cap = cv2.VideoCapture(videoFile)
x=1; count = 0
while(cap.isOpened()):
    # Current frame number
    frameId = cap.get(1)
    ret, frame = cap.read()
    if (ret != True):
        break
    # Saves every 10th frame in the video, can be changed 
    if (frameId % 10 == 0): 
        filename ='temp/' + videoFile.split(' ')[0] +"_frame%d.jpg" % count
        count+=1
        cv2.imwrite(filename, frame)
cap.release()

"""Part 2: Prepare the model we chose, this is what ours will look like for now"""

# Uses the base model of pre-trained VGG16 model
base_model = VGG16(weights='imagenet', include_top=False)
model = load_model('weight.hdf5')

"""Part 3: Begin analyzing the frames!"""

# creating two lists to store predicted and actual tags
predict = []

images = glob("temp/*.jpg")
prediction_images = []
for i in range(len(images)):
    img = image.load_img(images[i], target_size=(224,224,3))
    img = image.img_to_array(img)
    img = img/255
    prediction_images.append(img)
prediction_images = np.array(prediction_images)
prediction_images = base_model.predict(prediction_images)
prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*512)
prediction = model.predict_proba(prediction_images)

# Get prediction values here
prediction = prediction.transpose()
drumming = prediction[0]
erratic = prediction[1]
pockets = prediction[2]
negative = prediction[3]
"""Part 4: Finally generate outputs!"""

# Get timestamps
clip = VideoFileClip(videoFile)
timestamps = np.arange(0.0, clip.duration, clip.duration/len(pockets))
if (len(timestamps) - len(pockets)) == 1:
    timestamps = np.delete(timestamps, -1)

# Plot here!
plt.title('Prediction results')
plt.xlabel('Time (seconds)')
plt.ylabel('Guess (%)')

plt.plot(timestamps, pockets, label="Pockets")
plt.plot(timestamps, erratic, label="Erratic")
plt.plot(timestamps, drumming, label="Drumming")
plt.plot(timestamps, negative, label="Neither/Other")

plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',
           ncol=3, mode="expand", borderaxespad=0.)

plt.savefig(videoFile.split('.')[0] + '_Predictions.png')

# Creates the json here!
import json
data1 = list(zip(timestamps, pockets))
data2 = list(zip(timestamps, erratic))
data3 = list(zip(timestamps, drumming))
data4 = list(zip(timestamps, negative))

data1 = [list([float(i) for i in ele]) for ele in data1] 
data2 = [list([float(i) for i in ele]) for ele in data2] 
data3 = [list([float(i) for i in ele]) for ele in data3]
data4 = [list([float(i) for i in ele]) for ele in data4]

dicttemp = {'pockets':data1,'erratic':data2,'drumming':data3,'negative':data4}
with open(videoFile.split('.')[0] + '_data.json', 'w') as f:
    json.dump(dicttemp, f)

# Clear temp images
while os.path.isdir ("temp"):
    shutil.rmtree ("temp", ignore_errors=True)
os.makedirs ("temp")